{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données\n",
    "data = pd.read_csv(r\"C:\\Users\\yzi\\OneDrive\\Bureau\\Travaux\\Projet_Classification_THBS\\database\\credit_card_fraud_corr.csv\", index_col=\"transactionId\")\n",
    "\n",
    "# Afficher les premières lignes du DataFrame\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacer les lettres \"C\" et \"M\" par des chiffres dans les colonnes \"nameOrig\" et \"nameDest\"\n",
    "def replace_first_letter(value):\n",
    "    if value.startswith('C'):\n",
    "        return '1' + value[1:]\n",
    "    elif value.startswith('M'):\n",
    "        return '2' + value[1:]\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "\n",
    "#data['nameOrig'] = data['nameOrig'].apply(replace_first_letter)\n",
    "#data['nameDest'] = data['nameDest'].apply(replace_first_letter)\n",
    "#data['nameOrig'] = data['nameOrig'].astype('float')\n",
    "#data['nameDest'] = data['nameDest'].astype('float')\n",
    "\n",
    "# Convertir une colonne en type 'category'\n",
    "data['isFraud'] = data['isFraud'].astype('category')\n",
    "\n",
    "# Créer un dictionnaire de correspondance entre les types et les valeurs numériques\n",
    "type_mapping = {'TRANSFER': 1, 'PAYMENT': 2, 'CASH_OUT': 3, 'CASH_IN': 4, 'DEBIT': 5}\n",
    "\n",
    "# Remplacer les valeurs de la colonne \"type\" par les valeurs numériques correspondantes\n",
    "data['type'] = data['type'].replace(type_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer les caractéristiques et la variable cible\n",
    "X = data.drop(\"isFraud\", axis=1)  # Utiliser \"isFraud\" au lieu de \"Class\"\n",
    "y = data[\"isFraud\"]\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Créer l'objet SMOTE\n",
    "smote = SMOTE(random_state=42, k_neighbors=4)\n",
    "\n",
    "# Appliquer SMOTE sur les données d'entraînement uniquement\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Définir les hyperparamètres à rechercher\n",
    "param_grid = {\n",
    "    'n_estimators': [ 1000, 1400],\n",
    "    'max_depth': [ 7, 9]\n",
    "}\n",
    "\n",
    "# Instancier le modèle XGBClassifier\n",
    "model = XGBClassifier(enable_categorical=True)\n",
    "\n",
    "# Créer l'objet GridSearchCV avec SMOTE\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "\n",
    "# Effectuer la recherche sur la grille avec les données équilibrées\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Afficher les meilleurs hyperparamètres\n",
    "print(\"Meilleurs hyperparamètres trouvés:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Obtenir les meilleures performances\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Faire des prédictions sur les données de test\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Évaluer les performances du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Définir le nombre de plis pour la validation croisée\n",
    "n_splits = 10\n",
    "\n",
    "# Instancier la validation croisée stratifiée\n",
    "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Liste pour stocker les rapports de classification de chaque pli\n",
    "cv_classification_reports = []\n",
    "\n",
    "# Boucle sur les plis de la validation croisée\n",
    "for train_idx, val_idx in stratified_kfold.split(X, y):\n",
    "    # Diviser les données en ensembles d'entraînement et de validation\n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Appliquer SMOTE sur les données d'entraînement uniquement\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Entraîner le modèle sur l'ensemble d'entraînement\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Faire des prédictions sur l'ensemble de validation\n",
    "    y_pred_val = model.predict(X_val_fold)\n",
    "    \n",
    "    # Générer un rapport de classification pour ce pli\n",
    "    classification_report_fold = classification_report(y_val_fold, y_pred_val, output_dict=True)\n",
    "    \n",
    "    # Ajouter le rapport à la liste des rapports\n",
    "    cv_classification_reports.append(classification_report_fold)\n",
    "\n",
    "# Fusionner les rapports de tous les plis\n",
    "overall_classification_report = {\n",
    "    \"0\": {\"precision\": 0, \"recall\": 0, \"f1-score\": 0, \"support\": 0},\n",
    "    \"1\": {\"precision\": 0, \"recall\": 0, \"f1-score\": 0, \"support\": 0}\n",
    "}\n",
    "for report in cv_classification_reports:\n",
    "    for label in overall_classification_report.keys():\n",
    "        overall_classification_report[label][\"precision\"] += report[label][\"precision\"]\n",
    "        overall_classification_report[label][\"recall\"] += report[label][\"recall\"]\n",
    "        overall_classification_report[label][\"f1-score\"] += report[label][\"f1-score\"]\n",
    "        overall_classification_report[label][\"support\"] += report[label][\"support\"]\n",
    "\n",
    "# Calculer les moyennes des rapports de classification\n",
    "num_folds = len(cv_classification_reports)\n",
    "for label in overall_classification_report.keys():\n",
    "    overall_classification_report[label][\"precision\"] /= num_folds\n",
    "    overall_classification_report[label][\"recall\"] /= num_folds\n",
    "    overall_classification_report[label][\"f1-score\"] /= num_folds\n",
    "\n",
    "# Afficher le rapport de classification global\n",
    "print(\"Overall classification report:\")\n",
    "print(overall_classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Définir le nom du fichier pour sauvegarder le modèle\n",
    "filename = 'XGBmodel.pkl'\n",
    "\n",
    "# Utiliser pickle.dump() pour sauvegarder le modèle dans le fichier\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "print(\"Le modèle a été sauvegardé avec succès dans le fichier:\", filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Faire des prédictions sur les données de test\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculer la matrice de confusion\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Extraire les éléments de la matrice de confusion\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Calculer les statistiques du modèle\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Afficher la matrice de confusion et les statistiques du modèle\n",
    "print(\"Matrice de confusion:\")\n",
    "print(conf_matrix)\n",
    "print(\"Faux positifs:\", fp)\n",
    "print(\"Faux négatifs:\", fn)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Précision:\", precision)\n",
    "print(\"Rappel:\", recall)\n",
    "print(\"F1-score:\", f1_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
